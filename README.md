# Classifying Multimodal Data using Transformers: Applications to Municipal Issue Feedback in the Singapore Government

The increasing prevalence of multimodal data in our society has led to the increased need for machines to make sense of such data holistically However, data scientists and machine learning engineers aspiring to work on such data face challenges fusing the knowledge from existing tutorials which often deal with each mode separately. Drawing on our experience in classifying multimodal municipal issue feedback in the Singapore government, we conduct a hands-on tutorial to help flatten the learning curve for practitioners who want to apply machine learning to multimodal data.


Unfortunately, we are not able to conduct the tutorial using the municipal issue feedback data due to its sensitivity. Instead, we use a subset of the WebVision data. This dataset consists of labelled images, together with descriptions of them, crawled from the web. We chose this dataset because of its similar characteristics to our municipal issue feedback data (text descriptions correlate highly with the labels but associated images provide even better context).

In this tutorial, we teach participants how to classify multimodal data consisting of both text and images using Transformers. It
is targeted at an audience who have some familiarity with neural networks and are comfortable with writing code.

The outline of the tutorial is as follows:
1. Sharing of Experience: Municipal issue feedback classifi-
cation in the Singapore government
2. Text Classification: Train a text classification model using
BERT
3. Text and Image Classification (v1): Train a text and image
classification model using BERT and ResNet-50
4. Text and Image Classification (v2): Train a text and image
classification model using Align before Fuse (ALBEF)
5. Question and Answer/Discussion

